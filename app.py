import os
import time
import json
import hashlib
import requests
import pandas as pd
import numpy as np
from loguru import logger
from sklearn.metrics.pairwise import cosine_similarity
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys
from typing import List, Dict, Tuple, Optional
import threading

# Configuration
CONFIG = {
    "ollama_host": "http://localhost:11434",
    "embedding_model": "nomic-embed-text:latest",
    "llm_model": "qwen3:latest",
    "csv_path": "dataset/freelancer_earnings_bd.csv",
    "log_file": "freelancer_rag_test.log",
    "max_context_length": 4096,
    "top_k_results": 3,
    "cache_dir": "cache",
    "max_workers": 2,
    "batch_size": 20,
    "request_delay": 0.1
}


class FreelancerRAG:
    def __init__(self):
        self._setup_logging()
        self._check_ollama_server()
        self._check_models()

        self.cache_hash = self._get_file_hash(CONFIG["csv_path"])
        self._init_cache_dirs()

        # Thread-safe –æ–ø–µ—Ä–∞—Ü–∏–∏
        self._lock = threading.Lock()

        if self._check_cache_exists():
            self._load_from_cache()
        else:
            self._process_csv_data()
            self._save_to_cache()

    def _setup_logging(self):
        logger.add(
            CONFIG["log_file"],
            rotation="10 days",
            level="INFO",
            format="{time} | {level} | {message}"
        )

    def _check_ollama_server(self):
        print("üîÑ –ü—Ä–æ–≤–µ—Ä—è—é —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Ollama...")
        try:
            response = requests.get(
                f"{CONFIG['ollama_host']}/api/version", timeout=10)
            response.raise_for_status()
            print("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –∫–æ–º–∞–Ω–¥–æ–π: ollama serve")
            sys.exit(1)

    def _check_models(self):
        print("üîÑ –ü—Ä–æ–≤–µ—Ä—è—é –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π...")
        try:
            response = requests.get(
                f"{CONFIG['ollama_host']}/api/tags", timeout=10)
            available_models = [model["name"]
                                for model in response.json().get("models", [])]

            missing_models = []
            for model in [CONFIG["embedding_model"], CONFIG["llm_model"]]:
                if model not in available_models:
                    missing_models.append(model)

            if missing_models:
                print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏: {missing_models}")
                print("–°–∫–∞—á–∞–π—Ç–µ –∏—Ö –∫–æ–º–∞–Ω–¥–∞–º–∏:")
                for model in missing_models:
                    print(f"  ollama pull {model}")
                sys.exit(1)

            print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            sys.exit(1)

    def _init_cache_dirs(self):
        os.makedirs(CONFIG["cache_dir"], exist_ok=True)

    def _get_file_hash(self, filepath):
        if not os.path.exists(filepath):
            print(f"‚ùå CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
            sys.exit(1)

        hasher = hashlib.md5()
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()

    def _check_cache_exists(self):
        cache_files = {
            'text': os.path.join(CONFIG["cache_dir"], f"text_cache_{self.cache_hash}.json"),
            'embeddings': os.path.join(CONFIG["cache_dir"], f"embeddings_cache_{self.cache_hash}.npy"),
            'metadata': os.path.join(CONFIG["cache_dir"], f"metadata_cache_{self.cache_hash}.json")
        }

        if all(os.path.exists(f) for f in cache_files.values()):
            self.text_cache_file = cache_files['text']
            self.embeddings_cache_file = cache_files['embeddings']
            self.metadata_cache_file = cache_files['metadata']
            return True
        return False

    def _load_from_cache(self):
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–µ—à–∞...")
        try:
            with open(self.text_cache_file, 'r', encoding='utf-8') as f:
                self.text_data = json.load(f)

            with open(self.metadata_cache_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)

            self.embeddings = np.load(self.embeddings_cache_file)

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.text_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫–µ—à–∞")
            print(
                f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.metadata.get('total_records', 0)} –∏—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
            self._process_csv_data()
            self._save_to_cache()

    def _save_to_cache(self):
        print("üíæ –°–æ—Ö—Ä–∞–Ω—è—é –¥–∞–Ω–Ω—ã–µ –≤ –∫–µ—à...")
        try:
            cache_files = {
                'text': os.path.join(CONFIG["cache_dir"], f"text_cache_{self.cache_hash}.json"),
                'embeddings': os.path.join(CONFIG["cache_dir"], f"embeddings_cache_{self.cache_hash}.npy"),
                'metadata': os.path.join(CONFIG["cache_dir"], f"metadata_cache_{self.cache_hash}.json")
            }

            with open(cache_files['text'], 'w', encoding='utf-8') as f:
                json.dump(self.text_data, f, ensure_ascii=False, indent=2)

            with open(cache_files['metadata'], 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)

            np.save(cache_files['embeddings'], self.embeddings)
            print("‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫–µ—à")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞: {e}")

    def _process_csv_data(self):
        print("üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é CSV –¥–∞–Ω–Ω—ã–µ...")
        try:
            # —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            encodings = ['utf-8', 'cp1251', 'iso-8859-1', 'utf-16']
            df = None

            for encoding in encodings:
                try:
                    df = pd.read_csv(CONFIG["csv_path"], encoding=encoding)
                    print(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {encoding}")
                    break
                except UnicodeDecodeError:
                    continue

            if df is None:
                raise Exception(
                    "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV —Å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞–º–∏")

            self.df = df
            print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.df)} –∑–∞–ø–∏—Å–µ–π")
            print(f"üìã –°—Ç–æ–ª–±—Ü—ã: {list(self.df.columns)}")

            # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            self.text_data = []

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.metadata = {
                'total_records': len(self.df),
                'columns': list(self.df.columns),
                'processing_time': time.time()
            }

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π
            numeric_cols = self.df.select_dtypes(include=[np.number]).columns
            stats = {}
            for col in numeric_cols:
                if self.df[col].notna().any():
                    stats[col] = {
                        'mean': float(self.df[col].mean()),
                        'std': float(self.df[col].std()),
                        'min': float(self.df[col].min()),
                        'max': float(self.df[col].max())
                    }

            self.metadata['stats'] = stats

            # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏
            for idx, row in self.df.iterrows():
                # –°–æ–∑–¥–∞–µ–º –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞
                description_parts = []

                # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if 'Job_Category' in row and pd.notna(row['Job_Category']):
                    description_parts.append(
                        f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ä–∞–±–æ—Ç—ã: {row['Job_Category']}")

                if 'Platform' in row and pd.notna(row['Platform']):
                    description_parts.append(f"–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞: {row['Platform']}")

                if 'Experience_Level' in row and pd.notna(row['Experience_Level']):
                    description_parts.append(
                        f"–£—Ä–æ–≤–µ–Ω—å –æ–ø—ã—Ç–∞: {row['Experience_Level']}")

                if 'Client_Region' in row and pd.notna(row['Client_Region']):
                    description_parts.append(
                        f"–†–µ–≥–∏–æ–Ω –∫–ª–∏–µ–Ω—Ç–∞: {row['Client_Region']}")

                # –§–∏–Ω–∞–Ω—Å–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if 'Hourly_Rate' in row and pd.notna(row['Hourly_Rate']):
                    rate = row['Hourly_Rate']
                    if 'Hourly_Rate' in stats:
                        if rate >= stats['Hourly_Rate']['mean'] + stats['Hourly_Rate']['std']:
                            rate_category = "–≤—ã—Å–æ–∫–∞—è"
                        elif rate <= stats['Hourly_Rate']['mean'] - stats['Hourly_Rate']['std']:
                            rate_category = "–Ω–∏–∑–∫–∞—è"
                        else:
                            rate_category = "—Å—Ä–µ–¥–Ω—è—è"
                        description_parts.append(
                            f"–ü–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞: ${rate} ({rate_category})")
                    else:
                        description_parts.append(f"–ü–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞: ${rate}")

                if 'Earnings_USD' in row and pd.notna(row['Earnings_USD']):
                    earnings = row['Earnings_USD']
                    if 'Earnings_USD' in stats:
                        if earnings >= stats['Earnings_USD']['mean'] + stats['Earnings_USD']['std']:
                            earnings_category = "–≤—ã—Å–æ–∫–∏–µ"
                        elif earnings <= stats['Earnings_USD']['mean'] - stats['Earnings_USD']['std']:
                            earnings_category = "–Ω–∏–∑–∫–∏–µ"
                        else:
                            earnings_category = "—Å—Ä–µ–¥–Ω–∏–µ"
                        description_parts.append(
                            f"–î–æ—Ö–æ–¥—ã: ${earnings} ({earnings_category})")
                    else:
                        description_parts.append(f"–î–æ—Ö–æ–¥—ã: ${earnings}")

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if 'Job_Success_Rate' in row and pd.notna(row['Job_Success_Rate']):
                    success_rate = row['Job_Success_Rate']
                    if success_rate >= 90:
                        success_category = "–æ—Ç–ª–∏—á–Ω—ã–π"
                    elif success_rate >= 70:
                        success_category = "—Ö–æ—Ä–æ—à–∏–π"
                    else:
                        success_category = "–Ω–∏–∑–∫–∏–π"
                    description_parts.append(
                        f"–†–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞: {success_rate}% ({success_category})")

                if 'Job_Duration_Days' in row and pd.notna(row['Job_Duration_Days']):
                    duration = row['Job_Duration_Days']
                    if duration <= 7:
                        duration_category = "–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–π"
                    elif duration <= 30:
                        duration_category = "—Å—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–π"
                    else:
                        duration_category = "–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π"
                    description_parts.append(
                        f"–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞: {duration} –¥–Ω–µ–π ({duration_category})")

                if 'Payment_Method' in row and pd.notna(row['Payment_Method']):
                    description_parts.append(
                        f"–°–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã: {row['Payment_Method']}")

                if 'Job_Completed' in row and pd.notna(row['Job_Completed']):
                    description_parts.append(
                        f"–ó–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç: {row['Job_Completed']}")

                # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è
                for col, val in row.items():
                    if (pd.notna(val) and
                        col not in ['Job_Category', 'Platform', 'Experience_Level', 'Client_Region',
                                    'Hourly_Rate', 'Earnings_USD', 'Job_Success_Rate', 'Job_Duration_Days',
                                    'Payment_Method', 'Job_Completed']):
                        description_parts.append(f"{col}: {val}")

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                if description_parts:
                    full_description = f"–§—Ä–∏–ª–∞–Ω—Å–µ—Ä #{idx + 1}: " + \
                        ". ".join(description_parts) + "."
                    self.text_data.append(full_description)
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                    self.text_data.append(
                        f"–§—Ä–∏–ª–∞–Ω—Å–µ—Ä #{idx + 1}: –ó–∞–ø–∏—Å—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.")

            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.text_data)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π")

            # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            if self.text_data:
                print(f"üìù –ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏:")
                print(f"   {self.text_data[0][:200]}...")

            self._generate_embeddings_parallel()

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV: {e}")
            logger.error(f"CSV processing error: {e}")
            sys.exit(1)

    def _generate_embeddings_parallel(self):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ThreadPoolExecutor"""
        print(
            f"üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤ {CONFIG['max_workers']} –ø–æ—Ç–æ–∫–∞—Ö...")

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã–¥–µ–ª—è–µ–º –ø–∞–º—è—Ç—å
        embeddings = [None] * len(self.text_data)
        total = len(self.text_data)
        completed = 0

        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        batch_size = CONFIG['batch_size']

        with ThreadPoolExecutor(max_workers=CONFIG['max_workers']) as executor:
            for batch_start in range(0, total, batch_size):
                batch_end = min(batch_start + batch_size, total)
                batch_texts = self.text_data[batch_start:batch_end]

                # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞
                future_to_index = {}
                for i, text in enumerate(batch_texts):
                    global_index = batch_start + i
                    future = executor.submit(
                        self._generate_embedding_safe, text, global_index)
                    future_to_index[future] = global_index

                # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞—Ç—á–∞
                for future in as_completed(future_to_index):
                    global_index = future_to_index[future]
                    try:
                        result = future.result(
                            timeout=120)  # –¢–∞–π–º–∞—É—Ç –Ω–∞ –∑–∞–¥–∞—á—É
                        if result is not None:
                            embeddings[global_index] = result

                        with self._lock:  # Thread-safe –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞
                            completed += 1
                            if completed % 10 == 0 or completed == total:
                                progress = (completed / total) * 100
                                print(
                                    f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed}/{total} ({progress:.1f}%)")

                    except Exception as e:
                        logger.error(
                            f"Error processing embedding for index {global_index}: {e}")
                        with self._lock:
                            completed += 1

                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                if batch_end < total:
                    time.sleep(1)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        valid_embeddings = []
        valid_texts = []

        for i, (text, embedding) in enumerate(zip(self.text_data, embeddings)):
            if embedding is not None:
                valid_embeddings.append(embedding)
                valid_texts.append(text)
            else:
                logger.warning(f"Missing embedding for text {i}")

        if not valid_embeddings:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞")
            sys.exit(1)

        self.embeddings = np.array(valid_embeddings)
        self.text_data = valid_texts

        print(
            f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(self.embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {total} –∑–∞–ø–∏—Å–µ–π")
        if len(self.embeddings) < total:
            print(
                f"‚ö†Ô∏è –ü–æ—Ç–µ—Ä—è–Ω–æ {total - len(self.embeddings)} –∑–∞–ø–∏—Å–µ–π –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")

    def _generate_embedding_safe(self, text: str, index: int) -> Optional[np.ndarray]:
        """Thread-safe –≤–µ—Ä—Å–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"""
        max_retries = 3
        base_delay = CONFIG['request_delay']

        for attempt in range(max_retries):
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞
                time.sleep(base_delay * (1 + np.random.random() * 0.5))

                truncated_text = text[:CONFIG["max_context_length"]]

                response = requests.post(
                    f"{CONFIG['ollama_host']}/api/embed",
                    json={
                        "model": CONFIG["embedding_model"],
                        "input": truncated_text
                    },
                    timeout=90
                )
                response.raise_for_status()

                result = response.json()
                if "embeddings" in result and result["embeddings"]:
                    return np.array(result["embeddings"][0])
                else:
                    logger.error(
                        f"No embeddings in response for index {index}: {result}")
                    return None

            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt) * \
                        (1 + np.random.random())
                    time.sleep(delay)
                    continue
                else:
                    logger.error(
                        f"Timeout after {max_retries} attempts for index {index}")
                    return None
            except Exception as e:
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    time.sleep(delay)
                    continue
                else:
                    logger.error(
                        f"Error generating embedding for index {index} after {max_retries} attempts: {e}")
                    return None

        return None

    def _generate_embedding(self, text: str) -> Optional[np.ndarray]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        return self._generate_embedding_safe(text, -1)

    def _find_similar(self, query: str, top_k: Optional[int] = None) -> List[Dict]:
        if top_k is None:
            top_k = CONFIG["top_k_results"]

        query_embedding = self._generate_embedding(query)
        if query_embedding is None:
            return []

        if len(self.embeddings) == 0:
            return []

        similarities = cosine_similarity([query_embedding], self.embeddings)[0]
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        results = []
        for i in top_indices:
            if i < len(self.text_data):
                results.append({
                    'text': self.text_data[i],
                    'similarity': similarities[i],
                    'index': i
                })

        return results

    def generate_answer(self, query: str) -> Tuple[str, float]:
        start_time = time.time()

        similar_items = self._find_similar(query, CONFIG["top_k_results"])
        if not similar_items:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.", time.time() - start_time

        # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = [f"–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {item['similarity']:.3f}):\n{item['text']}"
                         for item in similar_items[:3]]
        context = "\n\n".join(context_parts)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
        if hasattr(self, 'metadata') and 'stats' in self.metadata:
            stats_info = []
            for field, stat in self.metadata['stats'].items():
                if any(keyword in field.lower() for keyword in ['rate', 'earning', 'success']):
                    stats_info.append(
                        f"{field}: —Å—Ä–µ–¥–Ω–µ–µ={stat['mean']:.2f}, –º–∏–Ω={stat['min']:.2f}, –º–∞–∫—Å={stat['max']:.2f}")

            if stats_info:
                context += f"\n\n–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –¥–∞–Ω–Ω—ã–º:\n" + \
                    "\n".join(stats_info)

        messages = [
            {
                "role": "system",
                "content": (
                    "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö –æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞—Ö. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. "
                    "–ï—Å–ª–∏ —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç - —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ. "
                    "–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
                    "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. "
                    "–ï—Å–ª–∏ –≤–∏–¥–∏—à—å —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö - —É–∫–∞–∂–∏ –∏—Ö. "
                    "–í—Å–µ–≥–¥–∞ –æ—Å–Ω–æ–≤—ã–≤–∞–π –≤—ã–≤–æ–¥—ã –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–∞—Ö."
                )
            },
            {
                "role": "user",
                "content": f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {query}\n\n–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–π –æ—Ç–≤–µ—Ç:"
            }
        ]

        try:
            response = requests.post(
                f"{CONFIG['ollama_host']}/api/chat",
                json={
                    "model": CONFIG["llm_model"],
                    "messages": messages,
                    "stream": False
                },
                timeout=120
            )
            response.raise_for_status()

            result = response.json()
            if "message" in result and "content" in result["message"]:
                answer = result["message"]["content"]
            else:
                answer = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏."

        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            answer = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"

        return answer, time.time() - start_time

    def debug_info(self):
        """–í—ã–≤–æ–¥–∏—Ç –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("\n" + "="*60)
        print("üîç –û–¢–õ–ê–î–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
        print("="*60)

        if hasattr(self, 'metadata'):
            print(
                f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–º CSV: {self.metadata.get('total_records', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            print(f"üìã –°—Ç–æ–ª–±—Ü—ã: {', '.join(self.metadata.get('columns', []))}")

        print(
            f"üìù –¢–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π: {len(self.text_data) if hasattr(self, 'text_data') else 0}")
        print(
            f"üî¢ –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(self.embeddings) if hasattr(self, 'embeddings') else 0}")

        if hasattr(self, 'text_data') and self.text_data:
            print(f"\nüìÑ –ü–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–º–µ—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
            for i, text in enumerate(self.text_data[:3]):
                print(f"{i+1}. {text[:150]}...")

        if hasattr(self, 'metadata') and 'stats' in self.metadata:
            print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø–æ–ª—è–º:")
            for field, stats in self.metadata['stats'].items():
                print(
                    f"  {field}: –º–∏–Ω={stats['min']:.2f}, –º–∞–∫—Å={stats['max']:.2f}, —Å—Ä–µ–¥–Ω–µ–µ={stats['mean']:.2f}")

        print("="*60 + "\n")

    def run_sample_queries(self):
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –∑–∞–¥–∞–Ω–∏—è"""
        sample_queries = [
            "–ù–∞—Å–∫–æ–ª—å–∫–æ –≤—ã—à–µ –¥–æ—Ö–æ–¥ —É —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏—Ö –æ–ø–ª–∞—Ç—É –≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–µ, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –æ–ø–ª–∞—Ç—ã?",
            "–ö–∞–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ—Ö–æ–¥ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–≥–∏–æ–Ω–∞ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è?",
            "–ö–∞–∫–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤, —Å—á–∏—Ç–∞—é—â–∏—Ö —Å–µ–±—è —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, –≤—ã–ø–æ–ª–Ω–∏–ª –º–µ–Ω–µ–µ 100 –ø—Ä–æ–µ–∫—Ç–æ–≤?",
            "–ö–∞–∫–∞—è —Å—Ä–µ–¥–Ω—è—è –ø–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ —É –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Ä–∞–∑–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –æ–ø—ã—Ç–∞?",
            "–ö–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à–∏–π –¥–æ—Ö–æ–¥ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞–º?",
            "–ö–∞–∫ –≤–ª–∏—è–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ –Ω–∞ —Ä–∞–∑–º–µ—Ä –∑–∞—Ä–∞–±–æ—Ç–∫–∞ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞?"
        ]

        print("\n" + "="*60)
        print("üî¨ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´ - –¢–ï–°–¢–û–í–´–ï –ó–ê–ü–†–û–°–´")
        print("="*60)

        for i, query in enumerate(sample_queries, 1):
            print(f"\nüìã –ó–∞–ø—Ä–æ—Å {i}: {query}")
            print("-" * 50)

            answer, duration = self.generate_answer(query)
            print(f"üí° –û—Ç–≤–µ—Ç: {answer}")
            print(f"‚è±Ô∏è –í—Ä–µ–º—è: {duration:.2f} —Å–µ–∫")

            if i < len(sample_queries):
                input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")

    def run(self):
        def signal_handler(sig, frame):
            print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)

        print("\n" + "="*60)
        print("üöÄ –°–ò–°–¢–ï–ú–ê –ê–ù–ê–õ–ò–ó–ê –î–û–•–û–î–û–í –§–†–ò–õ–ê–ù–°–ï–†–û–í")
        print("="*60)
        print("üí° –ö–æ–º–∞–Ω–¥—ã:")
        print("   ‚Ä¢ –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ –¥–∞–Ω–Ω—ã—Ö —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤")
        print("   ‚Ä¢ 'demo' –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        print("   ‚Ä¢ 'debug' –¥–ª—è –æ—Ç–ª–∞–¥–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        print("   ‚Ä¢ 'exit' –∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
        print("   ‚Ä¢ 'help' –¥–ª—è –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤")
        print("="*60 + "\n")

        while True:
            try:
                user_input = input("‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

                if user_input.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
                    print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break

                if user_input.lower() in ['demo', '–¥–µ–º–æ']:
                    self.run_sample_queries()
                    continue

                if user_input.lower() in ['debug', '–æ—Ç–ª–∞–¥–∫–∞']:
                    self.debug_info()
                    continue

                if user_input.lower() in ['help', '–ø–æ–º–æ—â—å']:
                    print("\nüìã –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:")
                    print("  ‚Ä¢ –ö–∞–∫–∞—è —Å—Ä–µ–¥–Ω—è—è –ø–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ —É –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤?")
                    print("  ‚Ä¢ –°–∫–æ–ª—å–∫–æ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä—ã –≤ –ê–≤—Å—Ç—Ä–∞–ª–∏–∏?")
                    print("  ‚Ä¢ –ö–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ?")
                    print("  ‚Ä¢ –ö–∞–∫ –æ–ø—ã—Ç –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–æ—Ö–æ–¥—ã?")
                    print("  ‚Ä¢ –ö–∞–∫–æ–π —Ä–µ–π—Ç–∏–Ω–≥ —É—Å–ø–µ—Ö–∞ —É –Ω–æ–≤–∏—á–∫–æ–≤?")
                    print("  ‚Ä¢ –°–∫–æ–ª—å–∫–æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–æ–±–∏–ª—å–Ω—ã–º –±–∞–Ω–∫–∏–Ω–≥–æ–º?")
                    print("  ‚Ä¢ –ù–∞—Å–∫–æ–ª—å–∫–æ –≤—ã—à–µ –¥–æ—Ö–æ–¥—ã –ø—Ä–∏ –æ–ø–ª–∞—Ç–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π?\n")
                    continue

                if not user_input:
                    continue

                print("üîç –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞...")
                answer, duration = self.generate_answer(user_input)

                print(f"\nüí° {answer}")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {duration:.2f} —Å–µ–∫\n")

            except KeyboardInterrupt:
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            except Exception as e:
                logger.error(f"Runtime error: {e}")
                print("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.\n")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    try:
        print("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Ö–æ–¥–æ–≤ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤...")
        rag_system = FreelancerRAG()
        rag_system.run()
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")
        sys.exit(1)


if __name__ == "__main__":
    main()
